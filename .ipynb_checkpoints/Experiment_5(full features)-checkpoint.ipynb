{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pyplot import figure\n",
    "from matplotlib.pyplot import suptitle\n",
    "import matplotlib.style as style\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "%config InlineBackend.figure_format = 'png' #set 'png' here when working on notebook\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "# sk learn import \n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold, RFE, SelectKBest, chi2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier, VotingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "# Set some parameters to get good visuals - style to ggplot and size to 15,10\n",
    "\n",
    "pd.set_option('display.width',170, 'display.max_rows',200, 'display.max_columns',900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/pvaish10/Desktop/TADPOLE_D1_D2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DX'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Data INFORMATION\")\n",
    "print (\"========================\\n\")\n",
    "\n",
    "\n",
    "#pd.isnull(df).sum() == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select all the columns with few missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[(pd.isnull(df).sum() == 0).index[(pd.isnull(df).sum() == 0)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop(['RID',\n",
    " 'PTID',\n",
    " 'VISCODE',\n",
    " 'SITE',\n",
    "'COLPROT',\n",
    " 'ORIGPROT',\n",
    " 'EXAMDATE',\n",
    " 'DX_bl',\n",
    "'update_stamp',\n",
    "                'EXAMDATE_bl',\n",
    "'VERSION_UCSFFSL_02_01_16_UCSFFSL51ALL_08_01_16',\n",
    " 'EXAMDATE_UCSFFSL_02_01_16_UCSFFSL51ALL_08_01_16',\n",
    "               'update_stamp_UCSFFSL_02_01_16_UCSFFSL51ALL_08_01_16',\n",
    " 'EXAMDATE_UCSFFSX_11_02_15_UCSFFSX51_08_01_16',\n",
    " 'VERSION_UCSFFSX_11_02_15_UCSFFSX51_08_01_16',\n",
    " 'LONISID_UCSFFSX_11_02_15_UCSFFSX51_08_01_16',\n",
    " 'FLDSTRENG_UCSFFSX_11_02_15_UCSFFSX51_08_01_16',\n",
    " 'LONIUID_UCSFFSX_11_02_15_UCSFFSX51_08_01_16',\n",
    " 'IMAGEUID_UCSFFSX_11_02_15_UCSFFSX51_08_01_16',\n",
    " 'RUNDATE_UCSFFSX_11_02_15_UCSFFSX51_08_01_16',\n",
    " 'STATUS_UCSFFSX_11_02_15_UCSFFSX51_08_01_16',\n",
    "               'update_stamp_UCSFFSX_11_02_15_UCSFFSX51_08_01_16',\n",
    " 'EXAMDATE_BAIPETNMRC_09_12_16',\n",
    " 'VERSION_BAIPETNMRC_09_12_16',\n",
    " 'LONIUID_BAIPETNMRC_09_12_16',\n",
    " 'RUNDATE_BAIPETNMRC_09_12_16',\n",
    " 'STATUS_BAIPETNMRC_09_12_16',\n",
    " 'update_stamp_BAIPETNMRC_09_12_16',\n",
    " 'EXAMDATE_UCBERKELEYAV45_10_17_16',\n",
    "               'update_stamp_UCBERKELEYAV45_10_17_16',\n",
    " 'EXAMDATE_UCBERKELEYAV1451_10_17_16',\n",
    "      'update_stamp_UCBERKELEYAV1451_10_17_16',\n",
    " 'EXAMDATE_DTIROI_04_30_14',\n",
    " 'VERSION_DTIROI_04_30_14',  \n",
    "               'RUNDATE_DTIROI_04_30_14',\n",
    " 'STATUS_DTIROI_04_30_14',\n",
    "               'update_stamp_DTIROI_04_30_14',\n",
    " 'EXAMDATE_UPENNBIOMK9_04_19_17',\n",
    " 'PHASE_UPENNBIOMK9_04_19_17',\n",
    " 'BATCH_UPENNBIOMK9_04_19_17',\n",
    " 'KIT_UPENNBIOMK9_04_19_17',\n",
    " 'STDS_UPENNBIOMK9_04_19_17',\n",
    " 'RUNDATE_UPENNBIOMK9_04_19_17',\n",
    " 'update_stamp_UPENNBIOMK9_04_19_17'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df1.columns)[1400:1405]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select rows which have a DX value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([df1, df['DX']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select rows which have a DX value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.loc[df2['DX'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make only three categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.replace({'NL to MCI': 'MCI', 'MCI to Dementia': 'Dementia', 'MCI to NL' : 'NL', 'NL to Dementia': 'Dementia', 'Dementia to MCI': 'MCI'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['DX'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = df2.DX[df2['D1'] == 1]\n",
    "test_y = df2.DX[df2['D2'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.isna().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.select_dtypes(include=['category','object_']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorial_cols = [\n",
    "    'PTGENDER', 'PTETHCAT', 'PTRACCAT', 'PTMARRY']\n",
    "\n",
    "for cc in categorial_cols:\n",
    "    dummies = pd.get_dummies(df2[cc])\n",
    "    dummies = dummies.add_prefix(\"{}#\".format(cc))\n",
    "    df2.drop(cc, axis=1, inplace=True)\n",
    "    df2 = df2.join(dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df2.columns\n",
    "df2[cols] = df2[cols].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['DX'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = df2[df2['D1'] == 1]\n",
    "train_X = train_X.drop(['D1','D2','DX'], 1)\n",
    "test_X = df2[df2['D2'] == 1]\n",
    "test_X = test_X.drop(['D1','D2','DX'], axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.DX[df2['D1'] == 1].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find all features with more than 90% variance in values.\n",
    "threshold = 0.90\n",
    "vt = VarianceThreshold().fit(train_X )\n",
    "\n",
    "# Find feature names\n",
    "feat_var_threshold = train_X.columns[vt.variances_ > threshold * (1-threshold)]\n",
    "feat_var_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_X, train_y)\n",
    "\n",
    "feature_imp = pd.DataFrame(model.feature_importances_, index=train_X.columns, columns=[\"importance\"])\n",
    "feat_imp_20 = feature_imp.sort_values(\"importance\", ascending=False).head(20).index\n",
    "feat_imp_20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate feature selection\n",
    "Select top 20 features using chi2chi2 test. Features must be positive before applying test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_minmax = MinMaxScaler(feature_range=(0,1)).fit_transform(train_X)\n",
    "X_scored = SelectKBest(score_func=chi2, k='all').fit(X_minmax, train_y)\n",
    "feature_scoring = pd.DataFrame({\n",
    "        'feature': train_X.columns,\n",
    "        'score': X_scored.scores_\n",
    "    })\n",
    "\n",
    "feat_scored_20 = feature_scoring.sort_values('score', ascending=False).head(20)['feature'].values\n",
    "feat_scored_20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive Feature Elimination\n",
    "Select 20 features from using recursive feature elimination (RFE) with logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFE(LogisticRegression(), 20)\n",
    "rfe.fit(train_X, train_y)\n",
    "\n",
    "feature_rfe_scoring = pd.DataFrame({\n",
    "        'feature': train_X.columns,\n",
    "        'score': rfe.ranking_\n",
    "    })\n",
    "\n",
    "feat_rfe_20 = feature_rfe_scoring[feature_rfe_scoring['score'] == 1]['feature'].values\n",
    "feat_rfe_20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final feature selection\n",
    "Finally features selected by all methods will be merged together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.hstack([\n",
    "        feat_var_threshold, \n",
    "        feat_imp_20,\n",
    "        feat_scored_20,\n",
    "        feat_rfe_20\n",
    "    ])\n",
    "\n",
    "features = np.unique(features)\n",
    "print('Final features set:\\n')\n",
    "for f in features:\n",
    "    print(\"\\t-{}\".format(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = train_X.ix[:, features]\n",
    "test_X = test_X.ix[:, features]\n",
    "\n",
    "print('Train features shape: {}'.format(train_X.shape))\n",
    "print('Target label shape: {}'. format(train_y.shape))\n",
    "print('Test features shape: {}'.format(test_X.shape))\n",
    "print('Target Test label shape: {}'. format(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = 8\n",
    "pca = PCA(n_components=components).fit(train_X)\n",
    "#Show explained variance for each component\n",
    "pca_variance_explained_df = pd.DataFrame({\n",
    "    \"component\": np.arange(1, components+1),\n",
    "    \"variance_explained\": pca.explained_variance_ratio_            \n",
    "    })\n",
    "\n",
    "ax = sns.barplot(x='component', y='variance_explained', data=pca_variance_explained_df)\n",
    "ax.set_title(\"PCA - Variance explained\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = pd.DataFrame(pca.transform(train_X)[:,:2])\n",
    "X_pca['target'] = train_y.values\n",
    "X_pca.columns = [\"x\", \"y\", \"target\"]\n",
    "\n",
    "sns.lmplot('x','y', \n",
    "           data=X_pca, \n",
    "           hue=\"target\", \n",
    "           fit_reg=False, \n",
    "           markers=[\"o\", \"x\"], \n",
    "           palette=\"Set1\", \n",
    "           size=7,\n",
    "           scatter_kws={\"alpha\": .2}\n",
    "          )\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute missing values after train test split\n",
    "# my_imputer = SimpleImputer()\n",
    "# train_X_imputed = pd.DataFrame(my_imputer.fit_transform(train_X.values))\n",
    "# test_X_imputed = pd.DataFrame(my_imputer.fit_transform(test_X.values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def multiclass_roc_dict(y_test,y_pred):\n",
    "    #creating a set of all the unique classes using the actual class list\n",
    "    unique_class = set(y_test.values)\n",
    "    roc_auc_dict = {}\n",
    "    for per_class in unique_class:\n",
    "        #creating a list of all the classes except the current class \n",
    "        other_class = [x for x in unique_class if x != per_class]\n",
    "\n",
    "        #marking the current class as 1 and all other classes as 0\n",
    "        new_actual_class = [0 if x in other_class else 1 for x in y_test]\n",
    "        new_pred_class = [0 if x in other_class else 1 for x in y_pred]\n",
    "\n",
    "        #using the sklearn metrics method to calculate the roc_auc_score\n",
    "        roc_auc = roc_auc_score(new_actual_class, new_pred_class, average = 'macro')\n",
    "        roc_auc_dict[per_class] = roc_auc\n",
    "    return roc_auc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "def get_multiclass_roc_dict(max_leaf_nodes, train_X, test_X, train_y, test_y):\n",
    "    model = DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "    model.fit(train_X, train_y)\n",
    "    preds_y = model.predict(test_X)\n",
    "    multiclass_roc = multiclass_roc_dict(test_y, preds_y)\n",
    "    return(multiclass_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Decision Tree results with different number of leaf nodes:\")\n",
    "for max_leaf_nodes in [5, 50, 500, 5000,50000]:\n",
    "    my_multiclass_roc_dict = get_multiclass_roc_dict(max_leaf_nodes, train_X, test_X, train_y, test_y)\n",
    "    print(\"Max leaf nodes: %d\" %(max_leaf_nodes))\n",
    "    print(my_multiclass_roc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "forest_model = RandomForestClassifier(random_state=99)\n",
    "forest_model.fit(train_X, train_y)\n",
    "preds_y = forest_model.predict(test_X)\n",
    "print(\"Random Forest Results\")\n",
    "print(multiclass_roc_dict(test_y, preds_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "my_pipeline = make_pipeline(SimpleImputer(),XGBClassifier())\n",
    "my_pipeline.fit(train_X, train_y)\n",
    "preds_y = my_pipeline.predict(test_X)\n",
    "print(\"XGBoost Results\")\n",
    "print(multiclass_roc_dict(test_y, preds_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost with parameters tuning \n",
    "xgb_model = XGBClassifier(n_estimators=1000)\n",
    "xgb_model.fit(train_X, train_y, verbose=False)\n",
    "preds_y = xgb_model.predict(test_X)\n",
    "print(\"XGBoost Results with Parameter Tuning\")\n",
    "print(multiclass_roc_dict(test_y, preds_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "# XGBoost with parameters tuning \n",
    "xgb_model = XGBClassifier(n_estimators=100)\n",
    "xgb_model.fit(train_X, train_y, verbose=False)\n",
    "preds_y = xgb_model.predict(test_X_imputed)\n",
    "print(\"Classification Report : {}\".format(classification_report(test_y, preds_y, labels=[1, 2, 3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import confusion_matrix\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.despine()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = unique_labels(df2['DX'])\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (8,6))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=['NL','MCI','Dementia'], yticklabels=['NL','MCI','Dementia'],\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=0, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(test_y, preds_y,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(test_y, preds_y, normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.despine()\n",
    "fig,ax = plt.subplots(figsize = (8,6))\n",
    "\n",
    "# Binarize the output\n",
    "y_bin = label_binarize(test_y, classes=[ 1, 2,3])\n",
    "n_classes = y_bin.shape[1]\n",
    "\n",
    "y_score = label_binarize(preds_y, classes=[1, 2,3])\n",
    "\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_bin[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "colors = ['blue', 'red', 'green']\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'\n",
    "             ''.format(i + 1, roc_auc[i] ))\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([-0.05, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic for Multi-class data (XGBoost)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretability and Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import xgboost\n",
    "\n",
    "# load JS visualization code to notebook\n",
    "shap.initjs()\n",
    "\n",
    "model = xgboost.train({\"learning_rate\": 0.01, \"n_estimators\": 100, \"eval_set\":[(test_X, test_y)] }, xgboost.DMatrix(train_X, train_y), 100)\n",
    "\n",
    "# explain the model's predictions using SHAP values\n",
    "# (same syntax works for LightGBM, CatBoost, and scikit-learn models)\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(train_X)\n",
    "\n",
    "# visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)\n",
    "shap.force_plot(explainer.expected_value, shap_values[0,:], train_X.iloc[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the training set predictions\n",
    "shap.force_plot(explainer.expected_value, shap_values, train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SHAP dependence plot to show the effect of a single feature across the whole dataset\n",
    "shap.dependence_plot(\"PTEDUCAT\", shap_values, train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SHAP dependence plot to show the effect of a single feature across the whole dataset\n",
    "shap.dependence_plot(\"AGE\", shap_values, train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the effects of all the features\n",
    "shap.summary_plot(shap_values, train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, train_X, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
